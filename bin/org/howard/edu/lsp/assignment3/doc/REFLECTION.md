In assignment 2, I created an ETL pipeline in a single class. This class contained all the logic for reading the given CSV file, transforming the rows, handling errors, and writing the output file. This solution worked, but it was very procedural and placed all responsibilities in one place. This created issues with modularity, reusability, and testing. 

In assignment 3, I redesigned my solution to make it more object-oriented. I deconstructed the pipeline into multiple classes and placed them in separate files, giving each class a single responsibility corresponding to the file/class name. On top of that, I added two classes: PipelineStats, which tracked row counts, and PipelineConfig, a class for configuration. Instead of the ETLPipeline class being the class containing all the logic, I made it the coordinator that delegates task to the other components instead of doing everything itself. This design was cleaner, more modular, and more maintainable. 

Assignment 3 is more object-oriented because it makes use of core ideas of object-oriented programming. The most obvious is the use of objects and classes to model real-world concepts. For example, Product respresents a data row, and CSVReader and CSVWriter represent actors in the pipeline. I also made use of encapsulation by hiding implementation details inside each class. You can see this in CSVReader, where I encapsulated the parsing of CSV lines or ProductTransformer which encapsulates rounding logic. Polymorphism is supported by the Transformer<T> interface, which allows the design to be extended to transform other kinds of objects in the future, not just products. Although this design does not make heavy use of inheritance, it lays a foundation for itâ€”for example, other types of transformers could subclass a base transformer if desired.

Testing was important to confirm that Assignment 3 worked the same way as Assignment 2. Since the requirements required identical inputs, outputs, transformations, and error handling, I compared the two versions directly. I used the same input file (data/products.csv) with valid rows, invalid rows, and edge cases such as missing fields or invalid prices. I verified that both versions produced the same output file (data/transformed_products.csv) with correctly rounded prices and skipped invalid rows. I also confirmed that the summary statistics (rows read, transformed, and skipped) matched exactly between Assignment 2 and Assignment 3. These checks gave me confidence that the refactoring preserved correctness while improving the design.

In conclusion, Assignment 3 demonstrates a clear progression from a procedural design to an object-oriented one. The pipeline became modular, more reusable, and easier to extend in the future. Even though both assignments produced the same results, the quality of the design improved significantly in Assignment 3 by applying object-oriented concepts such as encapsulation, polymorphism, and class-based modeling.  
